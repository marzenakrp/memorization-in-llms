# Research on memorization in LLMs

1. [**Quantifying Memorization Across Neural Language Models**](https://arxiv.org/abs/2202.07646) *Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang* ICLR 2023.

2. [**Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data**](https://www.arxiv.org/abs/2407.14985) *Antonis Antoniades, Xinyi Wang, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang Wang* ICML FM-Wild workshop 2024.

3. [**Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4**](https://aclanthology.org/2023.emnlp-main.453/) *Kent Chang, Mackenzie Cramer, Sandeep Soni, David Bamman* EMNLP 2023.

4. [**Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?**](https://arxiv.org/abs/2407.16607) *Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith* ArXiv 2024.

5. [**Time Travel in LLMs: Tracing Data Contamination in Large Language Models**](https://arxiv.org/abs/2308.08493) *Shahriar Golchin, Mihai Surdeanu* ICLR 2024.

